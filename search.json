[
  {
    "objectID": "pretrained-models.html",
    "href": "pretrained-models.html",
    "title": "Pretrained models",
    "section": "",
    "text": "Are these going to be links or do we want a page for each?",
    "crumbs": [
      "Pretrained models"
    ]
  },
  {
    "objectID": "Workshops/index.html",
    "href": "Workshops/index.html",
    "title": "Local Workshops",
    "section": "",
    "text": "Please see below for workshops taught locally at UW-Madison. To stay up-to-date on any new workshop offerings, we recommend subscribing to the Data Science @ UW newsletter.\nMost workshops listed have open source lesson materials which you are encouraged to independently study. If you are involved with a research lab on campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          offered\n        \n     \n  \n\n\n\n    \n        \n            \n                Intro to High-Dimensional Data Analysis\n                \n                    \n                    ML\n                    \n                    Regression\n                    \n                    High-Dimensional\n                    \n                \n                \n            \n                \n                    Organized by Data Science Hub, UW-Madison Carpentries\n                \n                \n                    Offered June 2024 |\n                \n                    \n                        Lesson Materials\n                     |\n                    \n                        Registration\n                    \n                \n                    12-16 hours to complete\n                \n            \n            \n            \n                Languages and Tools:\n                \n                Python\n                \n                Jupyter\n                \n            \n            \n            \n            \n                Packages and Libraries:\n                \n                sklearn\n                \n                pandas\n                \n                statsmodels\n                \n            \n            \n        \n        \n        \n            \n                Prerequisites\n                Introductory Python programming skills (variable assignments, how to create a function, for loops, etc.) and\nfamiliarity with the Pandas package.\nIf you need a refresher on Python before taking this workshop,\nplease review the lesson materials from this\n[Introductory Python Carpentries](https://swcarpentry.github.io/python-novice-inflammation/index.html) workshop.  \n\nFamiliarity with basic machine learning concepts including train/test splits and overfitting.\nFor a refresher on machine learning basics, please review the lesson materials from\nthe [Intro to Machine Learning with Sklearn](https://carpentries-incubator.github.io/machine-learning-novice-sklearn/)\nworkshop.\n\n            \n        \n        \n    \n        \n            \n                Intermediate Research Software Development with Python\n                \n                    \n                    Unit Testing\n                    \n                    Virtual Environments\n                    \n                    Version Control\n                    \n                \n                \n            \n                \n                    Organized by Data Science Hub, UW-Madison Carpentries\n                \n                \n                    Offered June 2024 |\n                \n                    \n                        Lesson Materials\n                     |\n                    \n                        Registration\n                    \n                \n                    18 hours to complete\n                \n            \n            \n            \n                Languages and Tools:\n                \n                PyCharm\n                \n                Git\n                \n                Unix Shell\n                \n            \n            \n            \n            \n                Packages and Libraries:\n                \n                numpy\n                \n                pytest\n                \n                venv\n                \n                poetry\n                \n            \n            \n        \n        \n        \n            \n                Prerequisites\n                Before joining this training, participants should meet the following criteria.\n(You can use this short quiz to test your prerequisite knowledge.)\n\nGit\nYou are familiar with the concept of version control\nYou have experience configuring Git for the first time and creating a local repository\nYou have experience using Git to create and clone a repository and add/commit changes to it and to push to/pull from a remote repository\nOptionally, you have experience comparing various versions of tracked files or ignoring specific files\n\nPython\nYou have a basic knowledge of programming in Python (using variables, lists, conditional statements, functions and importing external libraries)\nYou have previously written Python scripts or iPython/Jupyter notebooks to accomplish tasks in your domain of work\n\nShell\nYou have experience using a command line interface, such as Bash, to navigate a UNIX-style file system and run commands with arguments\nOptionally, you have experience redirecting inputs and outputs from a command\n\n            \n        \n        \n    \n        \n            \n                Intro to Deep Learning with Keras\n                \n                    \n                    ML\n                    \n                    Deep Learning\n                    \n                    Keras\n                    \n                \n                \n            \n                \n                    Organized by Data Science Hub, UW-Madison Carpentries\n                \n                \n                    Offered May 2024 |\n                \n                    \n                        Lesson Materials\n                     |\n                    \n                        Registration\n                    \n                \n                    12 hours to complete\n                \n            \n            \n            \n                Languages and Tools:\n                \n                Python\n                \n                Jupyter\n                \n            \n            \n            \n            \n                Packages and Libraries:\n                \n                keras\n                \n            \n            \n        \n        \n        \n            \n                Prerequisites\n                Basic Python programming skills and familiarity with the Pandas package. \n\nBasic knowledge on Machine learning, including the following concepts:\nData cleaning, train & test split, type of problems (regression, classification), overfitting & underfitting, metrics (accuracy, recall, etc.).\n\n            \n        \n        \n    \n        \n            \n                How to Analyze Data Using Python\n                \n                    \n                \n                \n            \n                \n                \n                    Offered March 2024 |\n                \n                    \n                        Lesson Materials\n                     |\n                    \n                        Registration\n                    \n                \n            \n            \n            \n        \n        \n    \n        \n            \n                How to Analyze Data Using R\n                \n                    \n                \n                \n            \n                \n                \n                    Offered March 2024 |\n                \n                    \n                        Lesson Materials\n                     |\n                    \n                        Registration\n                    \n                \n            \n            \n            \n        \n        \n    \n        \n            \n                Intro to Machine Learning with Scikit Learn\n                \n                    \n                    ML\n                    \n                \n                \n            \n                \n                    Organized by Data Science Hub, UW-Madison Carpentries\n                \n                \n                    Offered September 2023 |\n                \n                    \n                        Lesson Materials\n                     |\n                    \n                        Registration\n                    \n                \n                    8 hours to complete\n                \n            \n            \n            \n                Languages and Tools:\n                \n                Python\n                \n                Jupyter\n                \n            \n            \n            \n            \n                Packages and Libraries:\n                \n                sklearn\n                \n                pandas\n                \n            \n            \n        \n        \n        \n            \n                Prerequisites\n                A basic understanding of Python. You will need to know how to write a for loop, if statement, use functions, libraries and perform basic arithmetic. Either of the Software Carpentry Python courses cover sufficient background (e.g., https://swcarpentry.github.io/python-novice-inflammation/index.html)\n\n            \n        \n        \n    \n\n\nNo matching items",
    "crumbs": [
      "Local workshops"
    ]
  },
  {
    "objectID": "Seminars/mlx_2023-10-10.html",
    "href": "Seminars/mlx_2023-10-10.html",
    "title": "ML+X Forum: Time-Series Analysis",
    "section": "",
    "text": "Hosted by UW-Madison’s ML+X Community, each monthly forum highlights two machine learning applications that share a theme followed by communal discussions.\n\nComputational Methods for Comparative Time Clocks in Early Development and Tissue Regeneration, Peng Jiang\nControlled Differential Equations on Long Sequences via Non-standard Wavelets, Sourav Pal"
  },
  {
    "objectID": "Seminars/index.html",
    "href": "Seminars/index.html",
    "title": "Forums and seminars",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\nML+X Forum: Exploring Model Sharing in the Age of Foundation Models\n\n\n\n\n\n\n\n\n\n\n\n2024-03-12\n\n\nChris Endemann, Haotian Liu\n\n\n\n\n\n\n\nML+X Forum: Navigating Gravitational Waves with AI Insights\n\n\n\n\n\n\n\n\n\n\n\n2024-02-13\n\n\nChris Endemann, Bella Finkel\n\n\n\n\n\n\n\nML+X Forum: Exploring Science Communication and Drug Synergy Analysis using GPT\n\n\n\n\n\n\n\n\n\n\n\n2023-12-12\n\n\nBen Rush, Jack Freeman\n\n\n\n\n\n\n\nML+X Forum: LLMS in Genomic and Health Coaching\n\n\n\n\n\n\n\n\n\n\n\n2023-11-07\n\n\nRohan Sonthalia, Michael Roytman\n\n\n\n\n\n\n\nML+X Forum: Time-Series Analysis\n\n\n\n\n\n\n\n\n\n\n\n2023-10-10\n\n\nPeng Jiang, Sourav Pal\n\n\n\n\n\n\n\nML+X Forum: Multimodal Learning\n\n\n\n\n\n\n\n\n\n\n\n2023-09-19\n\n\nDaifeng Wang, Zachary Huemann, Pedro Morgado\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Forums and seminars"
    ]
  },
  {
    "objectID": "Seminars/mlx_2023-09-19.html",
    "href": "Seminars/mlx_2023-09-19.html",
    "title": "ML+X Forum: Multimodal Learning",
    "section": "",
    "text": "Hosted by UW-Madison’s ML+X Community, each monthly forum highlights 2-3 machine learning applications that share a theme followed by communal discussions.\n\nMultimodal learning and analysis for understanding single-cell functional genomics in brains and brain diseases, Daifeng Wang\nTransforming healthcare: AI-enhanced disease quantification with vision-language models, Zachary Huemann\nThe benefits of early fusion: deeply integrated audio-visual representation learning, Pedro Morgado"
  },
  {
    "objectID": "ML-stories/index.html",
    "href": "ML-stories/index.html",
    "title": "ML Stories",
    "section": "",
    "text": "Are you currently immersed in an exciting ML project? We want to hear about it! Share your insights, challenges, and successes by contributing a blog post to Nexus, the ML+X resource sharing platform.\nWhether you’re exploring ML applications in biology, engineering, social sciences, or any other field, your unique perspective is invaluable. Showcase your innovation, research, and creativity to inspire others in the ML+X community.\nDon’t miss this opportunity to amplify your work and connect with fellow enthusiasts. Submit your blog post today and be part of the ML+X conversation at UW-Madison!\nGet started by [insert contribution instructions].\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "ML stories"
    ]
  },
  {
    "objectID": "EDA-examples/index.html",
    "href": "EDA-examples/index.html",
    "title": "Exploratory analysis",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nEDA example\n\n\n\n\n\nAnother EDA example\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nAn Example\n\n\n\n\n\nAn exploratory data analysis example \n\n\n\n\n\n1 min\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Exploratory analysis"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary of ML terms",
    "section": "",
    "text": "Term\nMeaning\n\n\n\n\nML\nMachine learning\n\n\nEDA\nExploratory data analysis\n\n\nSVM\nSupport vector machines\n\n\nNLP\nNatural language processing\n\n\nPCA\nPrincipal component analysis\n\n\nCNN\nConvolutional neural network",
    "crumbs": [
      "Glossary of ML terms"
    ]
  },
  {
    "objectID": "Guides/Model-sharing/index.html",
    "href": "Guides/Model-sharing/index.html",
    "title": "Model sharing via Hugging Face",
    "section": "",
    "text": "Model Hub: https://huggingface.co/models\nNavigating the Model Hub: https://huggingface.co/docs/hub/en/models-the-hub\nModel cards\n\nhttps://huggingface.co/docs/hub/en/model-cards\nhttps://huggingface.co/blog/model-cards\n\nUploading a model to Hugging Face: https://huggingface.co/docs/hub/en/models-uploading\n\n\n\n\n\nNavigating the Model Hub: https://huggingface.co/docs/hub/en/models-the-hub\nModels are uploaded with tags associated with\n\ntask\nlibrary (PyTorch, Tensorflow, etc.)\ntraining data\nlanguage (english, spanish, etc.)\nlicense\n\nClicking a model will show you its model card\n\n\n\n\n\nSee: https://huggingface.co/docs/hub/en/model-cards and https://huggingface.co/blog/model-cards\nThe information included in any given model cards is, unfortunately, not well standardized. However, it is best practice to include at least the following pieces of information\n\nInfo on base model (e.g., if your model is finetuned)\nInfo on common model variations\n\nHelp others understand your model within the context of recent developments\n\nModel description\n\ngeneral purpose\narchitecture\n\nInfo on training data\n\nHow the data was collected\nData license and usage terms\nBasic descriptive statistics: number of samples, features, classes, etc. Describe and/or visualize data distribution.\nNote any class imbalance issues. Include assessments of bias and fairness, when possible.\n\nExample: The training dataset used for the facial recognition model might unintentionally be biased, containing predominantly images of people from certain demographic groups (e.g., predominantly light-skinned individuals).\n\n\nThe model’s evaluation results\nIntended uses and limitations\n\nThis is critical for all models, but especially models that can have a downstream impact on people.\nExample: If the model is not designed to handle diverse demographics, it may learn patterns that favor certain groups while performing poorly on others. In this case, the model may have higher accuracy for light-skinned individuals but lower accuracy for darker-skinned individuals.\n\n\n\n\n\n\n\nUploading a model to Hugging Face: https://huggingface.co/docs/hub/en/models-uploading\nAll models are stored as GitHub repositories\nSome frameworks (e.g., PyTorch) have integrations that make it straightforward to upload a model within your model training script\nYou get to decide what information and metadata you want to include in the GitHub repository\n\n\n\n\n\nIs there any way to help control the overall coding environment when sharing a model? For instance, if my goal is to reproduce paper results exactly rather than repurpose a model.\nHow documented should my data be within my model card? Should I save the details for a separate “data card”?\nIf you already have a convenient way to download and load models stored elsewhere (e.g. Zenodo), how much additional benefit is there to refactoring your code to use Hugging Face to host the models?\nWhat is involved in running an interactive demo on the Hugging Face website with your model?",
    "crumbs": [
      "Guides",
      "Model sharing via Hugging Face"
    ]
  },
  {
    "objectID": "Guides/Model-sharing/index.html#links",
    "href": "Guides/Model-sharing/index.html#links",
    "title": "Model sharing via Hugging Face",
    "section": "",
    "text": "Model Hub: https://huggingface.co/models\nNavigating the Model Hub: https://huggingface.co/docs/hub/en/models-the-hub\nModel cards\n\nhttps://huggingface.co/docs/hub/en/model-cards\nhttps://huggingface.co/blog/model-cards\n\nUploading a model to Hugging Face: https://huggingface.co/docs/hub/en/models-uploading",
    "crumbs": [
      "Guides",
      "Model sharing via Hugging Face"
    ]
  },
  {
    "objectID": "Guides/Model-sharing/index.html#navigating-model-hub",
    "href": "Guides/Model-sharing/index.html#navigating-model-hub",
    "title": "Model sharing via Hugging Face",
    "section": "",
    "text": "Navigating the Model Hub: https://huggingface.co/docs/hub/en/models-the-hub\nModels are uploaded with tags associated with\n\ntask\nlibrary (PyTorch, Tensorflow, etc.)\ntraining data\nlanguage (english, spanish, etc.)\nlicense\n\nClicking a model will show you its model card",
    "crumbs": [
      "Guides",
      "Model sharing via Hugging Face"
    ]
  },
  {
    "objectID": "Guides/Model-sharing/index.html#model-cards",
    "href": "Guides/Model-sharing/index.html#model-cards",
    "title": "Model sharing via Hugging Face",
    "section": "",
    "text": "See: https://huggingface.co/docs/hub/en/model-cards and https://huggingface.co/blog/model-cards\nThe information included in any given model cards is, unfortunately, not well standardized. However, it is best practice to include at least the following pieces of information\n\nInfo on base model (e.g., if your model is finetuned)\nInfo on common model variations\n\nHelp others understand your model within the context of recent developments\n\nModel description\n\ngeneral purpose\narchitecture\n\nInfo on training data\n\nHow the data was collected\nData license and usage terms\nBasic descriptive statistics: number of samples, features, classes, etc. Describe and/or visualize data distribution.\nNote any class imbalance issues. Include assessments of bias and fairness, when possible.\n\nExample: The training dataset used for the facial recognition model might unintentionally be biased, containing predominantly images of people from certain demographic groups (e.g., predominantly light-skinned individuals).\n\n\nThe model’s evaluation results\nIntended uses and limitations\n\nThis is critical for all models, but especially models that can have a downstream impact on people.\nExample: If the model is not designed to handle diverse demographics, it may learn patterns that favor certain groups while performing poorly on others. In this case, the model may have higher accuracy for light-skinned individuals but lower accuracy for darker-skinned individuals.",
    "crumbs": [
      "Guides",
      "Model sharing via Hugging Face"
    ]
  },
  {
    "objectID": "Guides/Model-sharing/index.html#uploading-a-model",
    "href": "Guides/Model-sharing/index.html#uploading-a-model",
    "title": "Model sharing via Hugging Face",
    "section": "",
    "text": "Uploading a model to Hugging Face: https://huggingface.co/docs/hub/en/models-uploading\nAll models are stored as GitHub repositories\nSome frameworks (e.g., PyTorch) have integrations that make it straightforward to upload a model within your model training script\nYou get to decide what information and metadata you want to include in the GitHub repository",
    "crumbs": [
      "Guides",
      "Model sharing via Hugging Face"
    ]
  },
  {
    "objectID": "Guides/Model-sharing/index.html#open-questions",
    "href": "Guides/Model-sharing/index.html#open-questions",
    "title": "Model sharing via Hugging Face",
    "section": "",
    "text": "Is there any way to help control the overall coding environment when sharing a model? For instance, if my goal is to reproduce paper results exactly rather than repurpose a model.\nHow documented should my data be within my model card? Should I save the details for a separate “data card”?\nIf you already have a convenient way to download and load models stored elsewhere (e.g. Zenodo), how much additional benefit is there to refactoring your code to use Hugging Face to host the models?\nWhat is involved in running an interactive demo on the Hugging Face website with your model?",
    "crumbs": [
      "Guides",
      "Model sharing via Hugging Face"
    ]
  },
  {
    "objectID": "Guides/index.html",
    "href": "Guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nOut of distribution detection\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel sharing via Hugging Face\n\n\n3 min\n\n\nModel Sharing Via Hugging Face’s Model Hub\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Git and GitHub Desktop\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraph neural networks\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCHTC\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "Guides/CHTC-main/index.html",
    "href": "Guides/CHTC-main/index.html",
    "title": "CHTC",
    "section": "",
    "text": "Established in 2006, the Center for High Throughput Computing (CHTC) is committed to democratizing access to powerful computing resources across all research domains. High Throughput Computing (HTC) encompasses a set of principles and techniques designed to optimize computing resource utilization towards solving complex problems. When applied to scientific computing, HTC enhances resource efficiency, automation, and accelerates scientific breakthroughs, including those in machine learning.\nAre you a researcher at UW-Madison seeking to extend your computing capabilities beyond local resources, particularly for machine learning tasks? Request an account now to take advantage of the open computing services offered by the CHTC!\n\n\nExplore our collection of templates tailored for high throughput compute (HTC) systems utilizing GPUs, ideal for accelerating machine learning workflows. These templates streamline the process of job submission, maximizing the utilization of GPU resources for your computational tasks in machine learning. Dive into efficient computing with our GPU-based templates available on GitHub: CHTC GPU Templates\n\n\n\nVisit the CHTC Recipes Repository to discover a collection of common CHTC workflows or “recipes”, including those specifically geared towards machine learning tasks:\n\nPython\nR\nPyTorch\nAlphafold\n\n\n\n\nEmpower your machine learning research endeavors with containerization! Our comprehensive guides on Docker and Apptainer for HTC empower researchers to encapsulate their machine learning workflows, dependencies, and environments efficiently. Seamlessly integrate containers into your machine learning computing workflow for enhanced reproducibility and scalability. Explore our containerization guides:\n\nDocker Jobs Guide\nApptainer HTC Guide",
    "crumbs": [
      "Guides",
      "CHTC"
    ]
  },
  {
    "objectID": "Guides/CHTC-main/index.html#gpu-based-templates",
    "href": "Guides/CHTC-main/index.html#gpu-based-templates",
    "title": "CHTC",
    "section": "",
    "text": "Explore our collection of templates tailored for high throughput compute (HTC) systems utilizing GPUs, ideal for accelerating machine learning workflows. These templates streamline the process of job submission, maximizing the utilization of GPU resources for your computational tasks in machine learning. Dive into efficient computing with our GPU-based templates available on GitHub: CHTC GPU Templates",
    "crumbs": [
      "Guides",
      "CHTC"
    ]
  },
  {
    "objectID": "Guides/CHTC-main/index.html#chtc-recipes-repository",
    "href": "Guides/CHTC-main/index.html#chtc-recipes-repository",
    "title": "CHTC",
    "section": "",
    "text": "Visit the CHTC Recipes Repository to discover a collection of common CHTC workflows or “recipes”, including those specifically geared towards machine learning tasks:\n\nPython\nR\nPyTorch\nAlphafold",
    "crumbs": [
      "Guides",
      "CHTC"
    ]
  },
  {
    "objectID": "Guides/CHTC-main/index.html#container-guides",
    "href": "Guides/CHTC-main/index.html#container-guides",
    "title": "CHTC",
    "section": "",
    "text": "Empower your machine learning research endeavors with containerization! Our comprehensive guides on Docker and Apptainer for HTC empower researchers to encapsulate their machine learning workflows, dependencies, and environments efficiently. Seamlessly integrate containers into your machine learning computing workflow for enhanced reproducibility and scalability. Explore our containerization guides:\n\nDocker Jobs Guide\nApptainer HTC Guide",
    "crumbs": [
      "Guides",
      "CHTC"
    ]
  },
  {
    "objectID": "Guides/Graph-neural-nets/index.html",
    "href": "Guides/Graph-neural-nets/index.html",
    "title": "Graph neural networks",
    "section": "",
    "text": "@TODO: ADD short blurb about why this tutorial was chosen and how it was helpful, any limitations, any other info that could be helpful.\n\n\n\n@TODO: Add list of other links on this topic",
    "crumbs": [
      "Guides",
      "Graph neural networks"
    ]
  },
  {
    "objectID": "Guides/Graph-neural-nets/index.html#about-this-external-resource",
    "href": "Guides/Graph-neural-nets/index.html#about-this-external-resource",
    "title": "Graph neural networks",
    "section": "",
    "text": "@TODO: ADD short blurb about why this tutorial was chosen and how it was helpful, any limitations, any other info that could be helpful.\n\n\n\n@TODO: Add list of other links on this topic",
    "crumbs": [
      "Guides",
      "Graph neural networks"
    ]
  },
  {
    "objectID": "Guides/github-desktop/index.html",
    "href": "Guides/github-desktop/index.html",
    "title": "Intro to Git and GitHub Desktop",
    "section": "",
    "text": "Navigating the world of version control systems like Git can initially feel daunting, especially for those new to programming or collaborative software development projects. However, with the right tools and guidance, anyone can quickly grasp the essentials and begin leveraging the power of Git for efficient project management and collaboration. While Git commands can be run via a Unix shell, there are alternatives which are more friendly for beginners. In this guide, we’ll explore why GitHub Desktop serves as an ideal gateway to Git, along with a step-by-step walkthrough on essential Git terminology, setup procedures, tracking changes, collaboration workflows, and even managing Kaggle notebooks seamlessly. Whether you’re embarking on your first coding adventure or seeking to streamline your team’s development process, this guide aims to demystify Git and empower you with practical knowledge to navigate the Git landscape with confidence.\nAlso see the Local Workshops page for relevant hands-on training opportunities related to version control.",
    "crumbs": [
      "Guides",
      "Intro to Git and GitHub Desktop"
    ]
  },
  {
    "objectID": "Guides/github-desktop/index.html#essential-git-terminology",
    "href": "Guides/github-desktop/index.html#essential-git-terminology",
    "title": "Intro to Git and GitHub Desktop",
    "section": "Essential Git Terminology",
    "text": "Essential Git Terminology\n\nRepository == repo: A project that is tracked via git/GitHub\n\nRemote repo: A git project that is stored on GitHub\nLocal repo: A git project that has been downloaded to your local machine\n\nClone: Cloning is the process of making a copy of a remote repo on your local machine. This allows you to work on the project locally and perform tasks like commits, branches, and pulls.\nCommit: A git command that marks the completion of new work to a repo (e.g., add a new script, add a feature, fill out README). You can always recover previous versions of your work by loading up a previous commit.\nPush: A git command that sends local changes (commits) stored in your local repo to the remote repo.\nPull: A git command that allows you to update your local repo based on changes made to the remote repo (e.g., if your colleague pushes to the remote repo)\nBranch: A branch in Git is a parallel line of development that allows you to work on features, bug fixes, or experiments without affecting the main codebase. You can create and switch between branches to isolate your work.\nMerge: Merging is the process of integrating changes from one branch into another. This is typically done to combine the changes made in a** feature branch** with the main branch (e.g., main or master).\nPull Request (PR): A pull request is a feature provided by platforms like GitHub, GitLab, and Bitbucket. It’s a way to propose changes (commits) to a project. Others can review the changes, and once approved, they can be merged into the main branch.\nFork: Forking a repository means creating a copy of someone else’s project in your GitHub account. This allows you to make changes independently and propose those changes back to the original project via pull requests. If everyone on your team has write-access to the repo, it’s best to use new branches instead of forks for pull requests.\nGitignore: A .gitignore file is used to specify which files and directories should be excluded from version control. It’s essential for preventing unnecessary or sensitive files (contains like API keys) from being included in the repository.",
    "crumbs": [
      "Guides",
      "Intro to Git and GitHub Desktop"
    ]
  },
  {
    "objectID": "Guides/github-desktop/index.html#setup",
    "href": "Guides/github-desktop/index.html#setup",
    "title": "Intro to Git and GitHub Desktop",
    "section": "Setup",
    "text": "Setup\n\nInstall GitHub Desktop\n\nVisit https://desktop.github.com/ to install\n\nCreate repo\n\nVisit https://github.com/ and sign in to your GitHub account\nClick “new” to create a new repo\nProvide a name for the project, e.g., “my_kaggle_project”\nGive a description: “Git repo for collaborating on Kaggle project for MLM23.”\nSet to private if you’re worried about having your work scooped\nAdd a README file: generally a good idea to have a README that explains how to use your code/repo\nChoose a license: https://choosealicense.com/. MIT license is usually best for open-source projects.\n\nAdd collaborator(s)\n\nFrom your repo homepage on GitHub, click the settings tab\nClick on the “Collaborators” menu option shown in the left panel\nClick “Add people” and enter your collaborator’s username or GitHub email address\n\nSetup SSH key: SSH provides a secure way to authenticate and transfer data between your local machine and GitHub. You can also use HTTPS if you prefer, but it is less secure. HTTPS avoids having to generate an SSH key, but you will need to enter your GitHub login credentials from time to time.\n\nOpen GitBash (windows) or terminal (Mac) and run the following commands replacing the example email with your GitHub email:\n\nssh-keygen -t ed25519 -C “your_github_email@address.com”\ncat ~/.ssh/id_ed25519.pub\nThe ssh-keygen produces private and public keys, and make sure to copy and paste the output from the command\ncat ~/.ssh/id_ed25519.pub\n\nPaste output (starts like ssh-ed25519) into the new SSH key under GitHub settings (SSH and GPG Keys) and save the key\n\nClone repo\n\nFrom your GitHub repo homepage, click the green “Code” button\nSelect SSH if you setup an SSH key or select HTTPS if you don’t have one setup. Copy the URL shown.\nOpen GitHub Desktop\nClick File → Clone repository → URL\nPaste the repo URL and pay attention to the destination folder path so you can access this folder later\nClick “Clone”",
    "crumbs": [
      "Guides",
      "Intro to Git and GitHub Desktop"
    ]
  },
  {
    "objectID": "Guides/github-desktop/index.html#tracking-changes",
    "href": "Guides/github-desktop/index.html#tracking-changes",
    "title": "Intro to Git and GitHub Desktop",
    "section": "Tracking changes",
    "text": "Tracking changes\n\nAdd a blank text file to your local repo\n\nRight-click repo name in GitHub Desktop → show in explorer (show in Finder and go to the directory on Mac)\nCreate a new text file and add to local repo folder\nAdd a line of text to the file, e.g., “hello world” and save the file\n\nView local changes \n\nIn GitHub desktop, you can view this change under the “Changes” tab. Notice that we see the new file and added text under this tab.\n\nCommit the new file\n\nCommits mark a checkpoint in the progress you have made to your repo. Provide a short summary message and optionally provide more information in the “Description” box.\n\nView remote changes (or lack thereof)\n\nVisit GitHub and notice that the change is not yet reflected on GitHub\n\nPush the change to GitHub \nView remote changes\n\nVisit GitHub again and notice the change has now been transferred to GitHub",
    "crumbs": [
      "Guides",
      "Intro to Git and GitHub Desktop"
    ]
  },
  {
    "objectID": "Guides/github-desktop/index.html#ignoring-.ipynb-files",
    "href": "Guides/github-desktop/index.html#ignoring-.ipynb-files",
    "title": "Intro to Git and GitHub Desktop",
    "section": "Ignoring .ipynb files",
    "text": "Ignoring .ipynb files\n\nAdd jupyter lab file to repo\n\nOpen anaconda prompt and cd into your local repo folder\nrun “jupyter lab” command to start a new jupyter lab instance\ncreate a new notebook, e.g., preprocess_data.ipynb\nadd a line of code, e.g., print(‘hello world’)\nsave the notebook and open GitHub desktop\n\nIn GitHub desktop, notice the changes being tracked are wildly confusing. \n\nJupyter files are stored in JSON format which includes a lot of metadata unrelated to the changes you made to your file. The solution? Use Jupytext!\n\nInstall jupytext\n\npip install jupytext\njupytext –set-formats ipynb,py *.ipynb # convert .ipynb files to .py\njupytext –set-formats py,ipynb *.py\n\nalternatively to convert just one specific file: jupytext –set-formats ipynb,py file_name.ipynb\n\n\ngit ignore .ipynb files\n\nright click one of the .ipynb files in GitHub Desktop\nignore all files of this type\n\ncommit changes\npush and view changes on GitHub",
    "crumbs": [
      "Guides",
      "Intro to Git and GitHub Desktop"
    ]
  },
  {
    "objectID": "Guides/github-desktop/index.html#pulling-updates-from-github",
    "href": "Guides/github-desktop/index.html#pulling-updates-from-github",
    "title": "Intro to Git and GitHub Desktop",
    "section": "Pulling updates from GitHub",
    "text": "Pulling updates from GitHub\nAssuming your collaborators have recently added some code to GitHub, how can you retrieve these changes and pull them onto your local machine?\n\nPretend you are a collaborator and visit GitHub to find your repo\nAdd a new file to the repo: Add file → create new file\nCommit the file to the repo\nOpen your local repo folder and notice we don’t have this new file yet\nIn GitHub Desktop, click “Fetch origin” by “Pull origin”\n\nFetch origin will run and inform you of any changes made to the remote copy of the repo (the one stored on GitHub)\nIf changes have been made since you last pulled, you’ll see the Fetch button turn into a “Pull” option. Click this option to retrieve any updates from GitHub and pull them into the local version of your repo.\n\nCheck your local repo folder to verify the new file has been pulled from GitHub onto your machine",
    "crumbs": [
      "Guides",
      "Intro to Git and GitHub Desktop"
    ]
  },
  {
    "objectID": "Guides/github-desktop/index.html#reverting-to-a-previous-commit",
    "href": "Guides/github-desktop/index.html#reverting-to-a-previous-commit",
    "title": "Intro to Git and GitHub Desktop",
    "section": "Reverting to a previous commit",
    "text": "Reverting to a previous commit\n\nFind the commit to revert to\n\nOpen the GitHub Desktop “History” tab to view the commit history of your repository.\nLocate and select the commit you want to revert to. You can do this by clicking on the specific commit in the history list.\n\nCreate a new branch from the selected commit\n\nWith the commit selected, click on the “Create branch” button. This action will create a new branch starting from the selected commit and automatically switch to this branch\n\nCommit the changes to the new branch\nMake any necessary changes in the new branch to resolve the issue or implement the desired changes. \nCommit and Push (“Publish branch”) \n\nIn GitHub Desktop, commit the changes to the branch and push them to the remote repository. This will upload the new branch and its commits to GitHub\n\nCreate a Pull Request (Optional)\n\nIf you want to merge these changes back into your main branch, you can create a pull request on GitHub. This is particularly useful if you are working with a team and need the changes reviewed.\n\nReview and Merge (If Using a Pull Request):\n\nIf you created a pull request, reviewers can examine your changes and merge them once they are satisfied.",
    "crumbs": [
      "Guides",
      "Intro to Git and GitHub Desktop"
    ]
  },
  {
    "objectID": "Guides/github-desktop/index.html#using-pull-requests-to-review-each-others-work",
    "href": "Guides/github-desktop/index.html#using-pull-requests-to-review-each-others-work",
    "title": "Intro to Git and GitHub Desktop",
    "section": "Using “pull requests” to review each other’s’ work",
    "text": "Using “pull requests” to review each other’s’ work\nPull requests provide a structured way for team members to review and collaborate on code changes. They allow peers to inspect, discuss, and provide feedback on the proposed modifications before merging them into the main codebase.\n\nCreate a New Branch: Click the “Current Branch” dropdown in GitHub Desktop. Select “New Branch” and give it a descriptive name (e.g., “feature-branch” or “collaborator-feature”). Choose the base branch, typically the default branch like main or master.\nMake Changes in the New Branch: Collaborators can now make changes in this new branch. They can create, edit, or delete files as needed.\nCommit and Push Changes: Collaborators should commit their changes to the new branch and push them to the remote repository.\nPreview Pull Request: In GitHub Desktop, click on “Preview Pull Request.” This will inform you of which branch is being merged into the main code base.\nCreate Pull Request: After confirming that the preview is correct, click “Create pull request”.\nReview and Submit the Pull Request on GitHub: Collaborators should review their changes and fill out the details for the pull request on the GitHub website. They can add a title, description, and assign reviewers (you and other collaborators) to review the changes. Collaborators can then submit the pull request.\nReview the Pull Request in GitHub Desktop: You and other collaborators can now go back to GitHub Desktop and see the newly created pull request listed in the “Current Branch” dropdown. Click on the pull request to view the changes, comments, and review the code. Collaborators can respond to any feedback or comments in the GitHub Desktop interface.\n** Accept or Request Changes**: After reviewing the code, you and other collaborators can either accept the pull request if it’s good to merge or request changes if there are issues that need to be addressed. You can leave comments, suggestions, and feedback in the pull request.\nCollaborators Make Changes: If changes are requested, collaborators can make the necessary adjustments in their branch and push the changes. The pull request will be automatically updated with the new commits.\nClose the Pull Request: Once the pull request is approved and the changes have been successfully reviewed, the pull request can be merged, and the branch can be deleted. Using this workflow, you can efficiently collaborate with collaborators who have write access to the repository while ensuring that changes are reviewed and merged using pull requests in a controlled and organized manner.",
    "crumbs": [
      "Guides",
      "Intro to Git and GitHub Desktop"
    ]
  },
  {
    "objectID": "Guides/github-desktop/index.html#saving-kaggle-notebooks-to-git",
    "href": "Guides/github-desktop/index.html#saving-kaggle-notebooks-to-git",
    "title": "Intro to Git and GitHub Desktop",
    "section": "Saving Kaggle Notebooks to Git",
    "text": "Saving Kaggle Notebooks to Git\nhttps://www.kaggle.com/discussions/product-feedback/295170",
    "crumbs": [
      "Guides",
      "Intro to Git and GitHub Desktop"
    ]
  },
  {
    "objectID": "how-to-contribute.html",
    "href": "how-to-contribute.html",
    "title": "How to contribute?",
    "section": "",
    "text": "Nexus is the ML+X community’s centralized resource hub for individuals interesting in advancing their knowledge and skill in machine learning (ML) and related fields (X). Moreover, we want Nexus to serve also as a place where members of the community can share their knowledge. This guide answers the question, how to contribute to the ML+X-Nexus?",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "how-to-contribute.html#how-to-write-the-post",
    "href": "how-to-contribute.html#how-to-write-the-post",
    "title": "How to contribute?",
    "section": "How to write the post?",
    "text": "How to write the post?\nTo write a post, there are many alternatives: Write it using quarto, rmarkdown, or jupyter. The post could be a new file in the folder, or a named folder with an index.[ipynb|qmd|rmd|md] extension. In any case, the header of the post needs to be a yaml section witht the fields:\n---\ntitle: An Example\ndescription: |\n  An exploratory data analysis example\nauthor: ML+X\ndate-modified: \"last-modified\"\ndate-format: long\ncategories:\n  - EDA\n  - PCA\n---\nThe only fields that need to be changed are title, description, author and the categories. Ideally the categories should match the tags that are already in use in the site, e.g. if tag that we are using for support vector machines is SVM then use that one instead of writing another one like support-vector-machines.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "how-to-contribute.html#sec-terminology",
    "href": "how-to-contribute.html#sec-terminology",
    "title": "How to contribute?",
    "section": "Extra: Essential Git Terminology:",
    "text": "Extra: Essential Git Terminology:\nThis section was copied from Chris’s guide on How to use Git/Github Demo\n\nRepository == repo: A project that is tracked via git/GitHub\n\nRemote repo: A git project that is stored on GitHub\nLocal repo: A git project that has been downloaded to your local machine\n\nClone: Cloning is the process of making a copy of a remote repo on your local machine. This allows you to work on the project locally and perform tasks like commits, branches, and pulls.\nCommit: A git command that marks the completion of new work to a repo (e.g., add a new script, add a feature, fill out README). You can always recover previous versions of your work by loading up a previous commit.\nPush: A git command that sends local changes (commits) stored in your local repo to the remote repo.\nPull: A git command that allows you to update your local repo based on changes made to the remote repo (e.g., if your colleague pushes to the remote repo)\nBranch: A branch in Git is a parallel line of development that allows you to work on features, bug fixes, or experiments without affecting the main codebase. You can create and switch between branches to isolate your work.\nMerge: Merging is the process of integrating changes from one branch into another. This is typically done to combine the changes made in a feature branch with the main branch (e.g., main or master).\nPull Request (PR): A pull request is a feature provided by platforms like GitHub, GitLab, and Bitbucket. It’s a way to propose changes (commits) to a project. Others can review the changes, and once approved, they can be merged into the main branch.\nFork: Forking a repository means creating a copy of someone else’s project in your GitHub account. This allows you to make changes independently and propose those changes back to the original project via pull requests. If everyone on your team has write-access to the repo, it’s best to use new branches instead of forks for pull requests.\nGitignore: A .gitignore file is used to specify which files and directories should be excluded from version control. It’s essential for preventing unnecessary or sensitive files from being included in the repository.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Seminars/mlx_2024-03-12.html",
    "href": "Seminars/mlx_2024-03-12.html",
    "title": "ML+X Forum: Exploring Model Sharing in the Age of Foundation Models",
    "section": "",
    "text": "Hosted by UW-Madison’s ML+X Community, each monthly forum highlights two machine learning applications that share a theme followed by communal discussions.\n\nModel sharing and reproducible ML, Chris Endemann, 0:29\nLLaVA-NeXT and model sharing, Haotian Liu, 12:24"
  },
  {
    "objectID": "Seminars/mlx_2024-02-13.html",
    "href": "Seminars/mlx_2024-02-13.html",
    "title": "ML+X Forum: Navigating Gravitational Waves with AI Insights",
    "section": "",
    "text": "Hosted by UW-Madison’s ML+X community, each monthly forum highlights two machine learning applications that share a theme followed by communal discussions.\n\nWelcome and small group discussions (discussion activity removed from recording), Chris Endemann\nClassifying gravitational wave modes from core-collapse supernovae, Bella Finkel, 1:34"
  },
  {
    "objectID": "Seminars/mlx_2023-11-07.html",
    "href": "Seminars/mlx_2023-11-07.html",
    "title": "ML+X Forum: LLMS in Genomic and Health Coaching",
    "section": "",
    "text": "Hosted by UW-Madison’s ML+X Community, each monthly forum highlights two machine learning applications that share a theme followed by communal discussions.\n\nClustering of genomic sequences of mycoviruses using deep learning, Rohan Sonthalia\nSpurring self-improvement and intrinsic motivation using LLMs and reinforcement learning, Michael Roytman"
  },
  {
    "objectID": "Seminars/mlx_2023-12-12.html",
    "href": "Seminars/mlx_2023-12-12.html",
    "title": "ML+X Forum: Exploring Science Communication and Drug Synergy Analysis using GPT",
    "section": "",
    "text": "Hosted by UW-Madison’s ML+X community, each monthly forum highlights two machine learning applications that share a theme followed by communal discussions.\n\nGPT for Science Communication: User-Interface and Developer Pipeline Approaches, Ben Rush, 0:10\nAdvancing Biomedical Research with GPT-4: A Novel Approach to Drug Synergy Analysis using Text Mining and Classification, Jack Freeman, 27:58"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML+X Nexus: Crowdsourced ML Resources",
    "section": "",
    "text": "Nexus is the ML+X community’s centralized hub for sharing machine learning (ML) resources and blogs that help make the practice of ML in Madison more connected, accessible, efficient, and reproducible.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#about-mlx",
    "href": "index.html#about-mlx",
    "title": "ML+X Nexus: Crowdsourced ML Resources",
    "section": "About ML+X",
    "text": "About ML+X\nML+X is a community of practice that brings together students, researchers, and industry professionals who share an interest in using machine learning (ML) methods (e.g., regression, classification, clustering, NLP, reinforcement learning, etc.) to advance their work (X). Community events and activities aim to help ML practitioners explore the challenges and pitfalls of ML, share knowledge and resources, and support each others’ work. Join the community to learn more!",
    "crumbs": [
      "Home"
    ]
  }
]